import os
import re
import json
import asyncio
from collections import defaultdict
from pathlib import Path
from typing import Optional, List, Dict
import time

import streamlit as st
import httpx
from dotenv import load_dotenv

from langchain_mistralai import ChatMistralAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

from vector_db import load_vector_db  # import relatif

# Configuration de la page Streamlit
st.set_page_config(
    page_title="SAWACARE AI CareManager",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS pour le design moderne
st.markdown("""
<style>
    /* Style g√©n√©ral */
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    /* Messages de chat */
    .user-message {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #2196f3;
    }
    
    .assistant-message {
        background-color: #f3e5f5;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #9c27b0;
    }
    
    .system-message {
        background-color: #e8f5e8;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #4caf50;
        font-style: italic;
    }
    
    /* Sidebar */
    .sidebar-content {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
    }
    
    /* Boutons */
    .stButton > button {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        font-weight: 600;
    }
    
    /* Chat input */
    .stTextInput > div > div > input {
        border-radius: 20px;
        border: 2px solid #e0e0e0;
        padding: 0.75rem 1rem;
    }
    
    /* M√©triques */
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        text-align: center;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# --- Configuration et initialisation ---
@st.cache_resource
def init_config():
    """Initialise la configuration depuis le .env"""
    PROJECT_ROOT = Path(__file__).resolve().parents[1]
    ENV_PATH = PROJECT_ROOT / ".env"
    load_dotenv(ENV_PATH, override=True)
    
    return {
        'API_KEY': os.getenv("API_KEY") or os.getenv("MISTRAL_API_KEY"),
        'MODEL_NAME': os.getenv("MODEL_NAME", "mistral-small-latest"),
        'K': 3,
        'TEMPERATURE': 0.0,
        'MAX_CONCURRENCY': int(os.getenv("MAX_CONCURRENCY", "4")),
        'MAX_TURNS': int(os.getenv("MAX_TURNS", "8")),
        'FALLBACK_MODELS': [
            os.getenv("MODEL_NAME", "mistral-small-latest"),
            "mistral-small", 
            "mistral-tiny"
        ]
    }

config = init_config()

# === R√©ponses fixes EXACTES ===
SALUTATION_REPLY = "Bonjour, comment puis-je vous aider ?"
GOODBYE_REPLY = "Merci au revoir"
WHOAMI_REPLY = (
    "Bonjour, je suis votre SAWACARE AI CareManager, votre assistant intelligent d√©di√© √† l'aidance. "
    "Je combine technologie et bienveillance pour vous aider √† mieux g√©rer vos responsabilit√©s d'aidant, "
    "√† trouver rapidement des r√©ponses fiables et √† b√©n√©ficier d'un accompagnement continu et personnalis√©."
)

# D√©tection (priorit√© : AU REVOIR > QUI √äTES-VOUS > SALUTATIONS)
GOODBYE_RE = re.compile(r"\b(au revoir|bonne (journ√©e|soir√©e|nuit)|√† bient√¥t|√† plus|merci|bye|ciao|tchao|see you|take care)\b", re.I)
WHOAMI_RE  = re.compile(r"(vous ?√™tes qui|qui ?√™tes[- ]?vous|t.?es qui|tu es qui|c.?est quoi ce bot|who are you)", re.I)
HELLO_RE   = re.compile(r"^\s*(bonjour|salut|hello|hi|hey|coucou|salam|bonsoir)\b", re.I)

def shortcut_reply(user_text: str) -> Optional[str]:
    """V√©rifie si le message correspond √† un raccourci"""
    t = (user_text or "").strip()
    if not t:
        return None
    if GOODBYE_RE.search(t):
        return GOODBYE_REPLY
    if WHOAMI_RE.search(t):
        return WHOAMI_REPLY
    if HELLO_RE.search(t):
        return SALUTATION_REPLY
    return None

# ---- Helpers ----
def require_api_key():
    """V√©rifie la pr√©sence de la cl√© API"""
    if not config['API_KEY']:
        st.error("‚ùå Cl√© Mistral absente. D√©finis API_KEY ou MISTRAL_API_KEY dans le .env.")
        st.stop()

def format_docs(docs) -> str:
    """Formate les documents r√©cup√©r√©s"""
    return "\n\n".join(d.page_content for d in docs)

def to_messages(raw_hist: Optional[List[Dict]], keep_last: int = None) -> List[BaseMessage]:
    """Convertit l'historique en messages LangChain"""
    if keep_last is None:
        keep_last = config['MAX_TURNS']
        
    msgs: List[BaseMessage] = []
    for m in (raw_hist or []):
        role = (m.get("role") or "").strip().lower()
        content = m.get("content", "")
        if not content:
            continue
        if role in ("user", "human"):
            msgs.append(HumanMessage(content=content))
        elif role in ("ai", "assistant", "bot"):
            msgs.append(AIMessage(content=content))
    return msgs[-keep_last:]

# ---- Vector store & retriever ----
@st.cache_resource
def init_vector_db():
    """Initialise la base de donn√©es vectorielle"""
    try:
        vector_db = load_vector_db()
        retriever = vector_db.as_retriever(search_kwargs={"k": config['K']})
        return retriever
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement de la base vectorielle: {e}")
        st.stop()

retriever = init_vector_db()

# ---- Prompt principal ----
prompt_template = ChatPromptTemplate.from_messages([
    ("system",
     "Tu es Sawa, l'assistant virtuel de l'application Sawa. "
     "Ta mission est d'aider les personnes √† simplifier leurs t√¢ches et √† les guider pas √† pas. "
     "R√©ponds STRICTEMENT √† partir du CONTEXTE fourni (issu du fichier de donn√©es / base). "
     "Si l'information n'est pas dans le contexte ou est insuffisante, dis clairement : "
     "¬´ Je n'ai pas assez d'informations dans le contexte pour r√©pondre. ¬ª "
     "N'invente jamais. "
     "Analyse soigneusement la question de l'utilisateur et r√©ponds pr√©cis√©ment en t'appuyant sur le contexte disponible. "
     "Assure une continuit√© de discussion en tenant compte de l'historique (notamment le dernier √©change) lorsqu'il est fourni. "
     "Quand il y a du contexte pertinent, termine toujours par 1‚Äì2 questions de suivi pour faciliter la poursuite de l'√©change. "
     "N'utilise PAS de formules de politesse (ex. ¬´ Bonjour ¬ª, ¬´ Merci ¬ª, ¬´ Cordialement ¬ª) √† chaque message : garde un ton respectueux et direct. "
     "N'emploie des salutations que si l'utilisateur en utilise explicitement ou pour clore d√©finitivement la conversation. "
     "Ne donne surtout pas d'avis m√©dical personnalis√©."
    ),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "Contexte (extraits de la base) :\n{context}\n\nQuestion : {question}")
])

# ---- LLM factory ----
@st.cache_resource
def make_llm(model_name: str) -> ChatMistralAI:
    """Cr√©e un objet LLM Mistral"""
    return ChatMistralAI(
        mistral_api_key=config['API_KEY'],
        model=model_name,
        temperature=config['TEMPERATURE'],
        max_retries=1,
    )

# --- Question rewriter ---
def build_question_rewriter(model_name: str):
    """Construit le r√©√©crivan de questions"""
    llm_local = make_llm(model_name)
    rewrite_prompt = ChatPromptTemplate.from_messages([
        ("system",
         "Tu reformules la derni√®re question utilisateur en une question autonome, "
         "en t'appuyant sur l'historique pour lever les pronoms, ellipses et r√©f√©rences implicites. "
         "Si l'historique est vide, renvoie la question telle quelle. "
         "R√©ponds UNIQUEMENT par la question r√©√©crite, sans explication."
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])
    return rewrite_prompt | llm_local | StrOutputParser()

# ---- Cha√Æne RAG ----
def build_chain_for(model_name: str):
    """Construit la cha√Æne RAG pour un mod√®le donn√©"""
    llm_local = make_llm(model_name)
    rewriter = build_question_rewriter(model_name)

    return (
        {
            "question": RunnableLambda(lambda x: x["question"]),
            "chat_history": RunnableLambda(lambda x: x.get("chat_history", [])),
        }
        | RunnablePassthrough.assign(
            standalone_q=RunnableLambda(lambda x: {
                "question": x["question"],
                "chat_history": x["chat_history"],
            }) | rewriter
        )
        | RunnablePassthrough.assign(
            context=RunnableLambda(lambda x: x["standalone_q"]) | retriever | RunnableLambda(format_docs)
        )
        | prompt_template
        | llm_local
        | StrOutputParser()
    )

def generate_with_fallback(payload: Dict) -> str:
    """G√©n√®re une r√©ponse avec syst√®me de fallback"""
    require_api_key()
    last_err = None
    
    for m in config['FALLBACK_MODELS']:
        try:
            chain = build_chain_for(m)
            return chain.invoke(payload)
        except httpx.HTTPStatusError as e:
            code = e.response.status_code if getattr(e, "response", None) else None
            if code in (429, 500, 502, 503, 504):
                last_err = e
                continue
            if code in (401, 403):
                st.error("üîë Appel Mistral refus√© (401/403). V√©rifie la cl√© API et les droits du mod√®le.")
                st.stop()
            st.error(f"üö® Erreur Mistral ({code}).")
            st.stop()
        except Exception as e:
            last_err = e
            continue
    
    st.error("üö´ Le service de g√©n√©ration est temporairement indisponible (fallback √©puis√©).")
    st.stop()

# ---- Interface Streamlit ----
def main():
    # Header principal
    st.markdown("""
        <div class="main-header">
            <h1>üè• SAWACARE AI CareManager</h1>
            <p>Votre assistant intelligent d√©di√© √† l'aidance</p>
        </div>
    """, unsafe_allow_html=True)

    # Sidebar avec informations
    with st.sidebar:
        st.markdown("### üìä Configuration")
        st.markdown(f"""
        <div class="sidebar-content">
            <strong>Mod√®le:</strong> {config['MODEL_NAME']}<br>
            <strong>Documents K:</strong> {config['K']}<br>
            <strong>Temp√©rature:</strong> {config['TEMPERATURE']}<br>
            <strong>Max tours:</strong> {config['MAX_TURNS']}
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("### üéØ Raccourcis")
        st.markdown("""
        <div class="sidebar-content">
            ‚Ä¢ <strong>Salutations:</strong> bonjour, salut, hello<br>
            ‚Ä¢ <strong>Qui √™tes-vous:</strong> qui √™tes-vous, you are<br>
            ‚Ä¢ <strong>Au revoir:</strong> au revoir, bye, merci
        </div>
        """, unsafe_allow_html=True)
        
        # Bouton pour vider l'historique
        if st.button("üóëÔ∏è Vider l'historique", use_container_width=True):
            if 'messages' in st.session_state:
                st.session_state.messages = []
            st.rerun()
        
        # Statistiques de session
        if 'messages' in st.session_state and st.session_state.messages:
            msg_count = len(st.session_state.messages)
            user_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
            assistant_msgs = len([m for m in st.session_state.messages if m["role"] == "assistant"])
            
            st.markdown("### üìà Statistiques")
            st.markdown(f"""
            <div class="sidebar-content">
                <strong>Total messages:</strong> {msg_count}<br>
                <strong>Vos messages:</strong> {user_msgs}<br>
                <strong>Mes r√©ponses:</strong> {assistant_msgs}
            </div>
            """, unsafe_allow_html=True)

    # Initialisation de l'√©tat de session
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Affichage de l'historique des messages
    st.markdown("### üí¨ Conversation")
    
    # Container pour les messages avec scroll
    messages_container = st.container()
    
    with messages_container:
        for message in st.session_state.messages:
            if message["role"] == "user":
                st.markdown(f"""
                <div class="user-message">
                    <strong>üë§ Vous:</strong><br>
                    {message["content"]}
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div class="assistant-message">
                    <strong>ü§ñ Sawa:</strong><br>
                    {message["content"]}
                </div>
                """, unsafe_allow_html=True)

    # Zone de saisie fix√©e en bas
    st.markdown("---")
    
    # Utilisation de colonnes pour le layout du chat
    col1, col2 = st.columns([4, 1])
    
    with col1:
        user_input = st.text_input(
            "Posez votre question...",
            key="user_input",
            placeholder="Tapez votre message ici...",
            label_visibility="collapsed"
        )
    
    with col2:
        send_button = st.button("üì§ Envoyer", use_container_width=True)

    # Traitement du message
    if send_button and user_input:
        # Ajouter le message utilisateur
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        # V√©rifier les raccourcis
        fixed_response = shortcut_reply(user_input)
        
        if fixed_response:
            # R√©ponse raccourci
            st.session_state.messages.append({"role": "assistant", "content": fixed_response})
            
            # Afficher la r√©ponse imm√©diatement
            st.markdown(f"""
            <div class="system-message">
                <strong>‚ö° R√©ponse automatique:</strong><br>
                {fixed_response}
            </div>
            """, unsafe_allow_html=True)
        else:
            # R√©ponse RAG avec spinner
            with st.spinner("ü§î Je r√©fl√©chis..."):
                try:
                    # Convertir l'historique
                    chat_history = to_messages([
                        {"role": m["role"], "content": m["content"]} 
                        for m in st.session_state.messages[:-1]  # Exclure le dernier message utilisateur
                    ])
                    
                    # Pr√©parer le payload
                    payload = {
                        "question": user_input,
                        "chat_history": chat_history
                    }
                    
                    # G√©n√©rer la r√©ponse
                    response = generate_with_fallback(payload)
                    
                    # Ajouter la r√©ponse √† l'historique
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    # Success message temporaire
                    success_placeholder = st.success("‚úÖ R√©ponse g√©n√©r√©e!")
                    time.sleep(1)
                    success_placeholder.empty()
                    
                except Exception as e:
                    st.error(f"‚ùå Erreur lors de la g√©n√©ration: {str(e)}")
        
        # Rerun pour afficher le nouveau message
        st.rerun()



if __name__ == "__main__":
    main()